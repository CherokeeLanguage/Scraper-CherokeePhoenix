package com.cherokeelessons.com.scraper.phoenix;
import java.io.File;
import java.io.IOException;
import java.net.HttpURLConnection;
import java.net.MalformedURLException;
import java.net.URL;
import java.text.NumberFormat;
import java.util.ArrayList;
import java.util.Collections;
import java.util.Date;
import java.util.List;
import java.util.Random;

import org.apache.commons.io.FileUtils;
import org.apache.commons.lang3.StringUtils;
import org.jsoup.Jsoup;
import org.jsoup.nodes.Document;
import org.jsoup.nodes.Element;
import org.jsoup.select.Elements;

public class Application extends Thread {
	final private long seconds = 1000;
	final private long minutes = 60 * seconds;
	final private long hours = 60 * minutes;

	private final long timeLimit = 4 * hours; // in ms

	private String baseURI = "http://www.cherokeephoenix.org";
	private String queryURIA = "/Article/Index/";

	private String[] args;
	private String destDir = "results";//set by cli option
	
	public Application(String[] args) {
		super();
		this.args = args;
	}

	@Override
	public void run() {
		try {
			_run();
		} catch (IOException e) {
			e.printStackTrace();
		}
	}

	private void _run() throws IOException {
		List<String> urlList;
		List<Article> articles=new ArrayList<Article>();
		
		urlList = loadSeedUrls();
		performHarvest(urlList);	
		
		HtmlCache.open();
		urlList=HtmlCache.allUrls();
		HtmlCache.close();
		
		System.err.println("========================================================");
		System.out.println("Processing "+urlList.size()+" cached articles.");
		System.err.println("========================================================");
		
		extractDataFromHtml(urlList, articles);
		System.err.println("Processing complete at " + new Date());
//		System.exit(0);
	}

	private void extractDataFromHtml(List<String> urlList,
			List<Article> listOfArticles) throws IOException {
		
		HtmlCache.open();

		System.err.println("PROCESSING HTML START: " + new Date());
		int size=urlList.size();
		
		for (int ix = 0; ix<size; ix++) {
			String articleUri = urlList.get(ix);
			String html = HtmlCache.getHtml(articleUri);
			if (html == null) {
				continue;
			}
			Article newArticle = new Article();
			newArticle.setHtml(html);
			if (!newArticle.isCherokee()){
				continue;
			}
			newArticle.setUri(articleUri);
			listOfArticles.add(newArticle);
		}
		HtmlCache.close();
		
		System.out.println("Found "+listOfArticles.size()+" Cherokee language articles.");
		System.err.println("PROCESSING HTML STOPPED: " + new Date());
		ArrayList<String> lines=new ArrayList<String>();
		String[] aLines;
		for (int ix=0; ix<listOfArticles.size(); ix++){
			lines.add("=========================================");
			Article b = listOfArticles.get(ix);
			if (b.getTitle_chr().length()>0) {
				lines.add(b.getTitle_chr());
			} else {
				lines.add(b.getTitle_en());
			}
			lines.add("=========================================");
			lines.add(b.getUri());
			if (b.getDate().length()>0) {
				lines.add("Date: "+b.getDate());
			}
			String c=b.getArticle_chr();
			aLines=c.split("\n\n+");
			for (int iy=0; iy<aLines.length; iy++) {
				String text=aLines[iy];
				text=text.replace("\n", " ");
				text=text.replaceAll("\t", " ");
				text=text.replaceAll(" +", " ");
				text=text.trim();
				lines.add(text);
			}
			lines.add("\n");
		}
		FileUtils.writeLines(new File("results/allArticles.txt"), "UTF-8", lines);
		
		lines.clear();
		System.out.println("Saving URLS as an HTML document w/titles.");
		lines.add("<html><head>");
		lines.add("<meta charset=\"UTF-8\" lang=\"chr\" />");
		lines.add("<title>ᏣᎳᎩ ᏧᎴᎯᏌᏅᎯ | ᏣᎳᎩ-ᏲᏁᎦ ᏗᎪᏪᎵ</title>");
		lines.add("<style>");
		lines.add("@import url(http://fonts.googleapis.com/earlyaccess/notosanscherokee.css);");
		lines.add("body {font-family: 'Noto Sans Cherokee', sans-serif;}");
		lines.add("</style>");
		lines.add("</head>");
		lines.add("<body>");
		lines.add("<h2>ᏣᎳᎩ ᏧᎴᎯᏌᏅᎯ | ᏣᎳᎩ-ᏲᏁᎦ ᏗᎪᏪᎵ</h2>");
		lines.add("<h3>Cherokee Phoenix | Cherokee-English Articles</h3>");
		lines.add("<p>This list was generated by Michael Joyner on "+new Date()+" using a custom scraping program written in Java</p>");
		lines.add("<p>A total of "+NumberFormat.getInstance().format(listOfArticles.size())+" dual-language articles were found.");
		lines.add("</p>");
		lines.add("<ol>");
		String d;
		String articleId;
		Collections.reverse(listOfArticles);
		for (Article article: listOfArticles) {
			lines.add("<li><a href=\""+article.getUri()+"\">");
			if (article.getDate().length()>1)  {
				d=" ("+article.getDate()+")";
			} else { 
				d="";
			}
			articleId=" ["+article.getUri().replaceAll("[^0-9]", "")+"]";
			String title = article.getTitle_chr();
			if (StringUtils.isBlank(title)) {
				title=article.getTitle_en();
			}
			lines.add(title);
			lines.add("</a>");
			lines.add(d+articleId);
			lines.add("</li>");
		}
		lines.add("</ol>");
		lines.add("</body></html>");
		FileUtils.writeLines(new File("results/ᏣᎳᎩ ᏧᎴᎯᏌᏅᎯ --- ᏣᎳᎩ-ᏲᏁᎦ ᏗᎪᏪᎵ.html"), "UTF-8", lines);
	}

	private void performHarvest(List<String> urlList) {
		HtmlCache.open();
		Document details;
		long startTime = System.currentTimeMillis();
		

		int lastPercent = -1;
		int newPercent = 0;
		String filingUri;
		String html;

		System.out.println("STARTING FETCH OF " + urlList.size() + " URLS.");
		System.err.println("HARVEST START: " + new Date());
//		boolean newData=false;
		for (int ix = 0; ix < urlList.size(); ix++) {
			if (System.currentTimeMillis() - startTime > timeLimit) {
				// out of time for harvesting, go on to next steps
				System.out.println("HARVEST Time limit reached.");
				break;
			}
			details = null;
			filingUri = urlList.get(ix);
			html = HtmlCache.getHtml(filingUri);
//			newData=false;
			if (html == null) {
				for (int retries = 0; retries < 3; retries++) {
					try {
						details = Jsoup.connect(filingUri).get();
						HtmlCache.putHtml(filingUri, details.outerHtml());
//						newData=true;
						html = details.outerHtml();
						break;
					} catch (IOException e) {
						if (e.getMessage().startsWith("404 error")){
							html="";
//							newData=false;
							break;
						} else {
							System.err.println(e.getMessage());
							System.err.println("RETRY: "+filingUri);
							randomSleep();
						}
					}
				}
			}
			if (html==null || html.length()<1) {
//				System.err.println("FAILED: "+filingUri);
				continue;
			}
			newPercent = (100 * ix / urlList.size());
			if (newPercent != lastPercent) {
				System.out.println("HARVEST " + newPercent + "% complete.");
				lastPercent = newPercent;
//				if (newData) randomSleep();
			}
		}

		// flush db to disk then reload
		System.err.println("HARVEST STOP: " + new Date());
		HtmlCache.close();
	}
	
	/**
	 * {@link http://stackoverflow.com/questions/4596447/check-if-file-exists-on-remote-server-using-its-url}
	 * @param URLName
	 * @return
	 */
	public static boolean httpExists(String URLName){
	    try {
	      HttpURLConnection.setFollowRedirects(false);
	      // note : you may also need
	      //        HttpURLConnection.setInstanceFollowRedirects(false)
	      HttpURLConnection con =
	         (HttpURLConnection) new URL(URLName).openConnection();
	      con.setRequestMethod("HEAD");
	      boolean b = con.getResponseCode() == HttpURLConnection.HTTP_OK;
//	      System.out.println("URLName: "+URLName+" ["+b+"]");
		return b;
	    }
	    catch (Exception e) {
	       e.printStackTrace();
	       return false;
	    }
	  }
	
	private List<String> loadSeedUrls() throws MalformedURLException, IOException {
		System.err.println("Loading initial URLS: " + new Date());
		ArrayList<String> urlList = new ArrayList<String>();
		int articleId=1;
		int prevArticleId=0;
		int failsInARow=0;
		String queryURI;
		
		HtmlCache.open();
		articleId=HtmlCache.getMaxArticleId();
		HtmlCache.close();
		int maxId=0;
		Document indexPage = Jsoup.parse(new URL("http://www.cherokeephoenix.org/"), 30000);
		Elements alist = indexPage.select("a");
		if (alist!=null) {
			for (Element a: alist) {
				if (!a.hasAttr("href")){
					continue;
				}
				String href=a.attr("href");
				if (!href.contains("Article/index/")){
					continue;
				}
				String number = StringUtils.substringAfterLast(href, "Article/index/");
				maxId=Math.max(Integer.valueOf(number), maxId);
			}
		}
		
		do {
			articleId++;
			queryURI = baseURI+queryURIA+String.valueOf(articleId);//+queryURIB;
			if (httpExists(queryURI)) {
				urlList.add(baseURI+queryURIA+String.valueOf(articleId));//+queryURIB);
				failsInARow=0;
				maxId=Math.max(maxId, articleId+100);
			} else {
				if (articleId>maxId) {
					failsInARow++;
				}
			}
			if (prevArticleId+99<articleId){
				System.out.println("Prescan at articleId: "+articleId);
				prevArticleId=articleId;
			}
		} while (failsInARow<100);

		System.out.println("CALCULATED URI LIST: (urls) "+urlList.size());
		
		System.err.println("Sorting and deduping URLS: " + new Date());
		// sort the list
		Collections.sort(urlList);
		// dedupe the sorted list
		for (int ix = urlList.size() - 1; ix > 0; ix--) {
			if (urlList.get(ix).equals(urlList.get(ix - 1))) {
				urlList.remove(ix);
			}
		}

		return urlList;
	}
	
	

	private void randomSleep() {
		randomSleep(1000, 0);
	}

	private void randomSleep(int msecs, int minSleep) {
		Random r = new Random();
		int sleep;
		if (msecs < minSleep) {
			msecs = minSleep;
		}
		sleep = r.nextInt(msecs);
		if (sleep < minSleep) {
			sleep = minSleep;
		}
		try {
			Thread.sleep(minSleep);
		} catch (InterruptedException e1) {
			e1.printStackTrace();
		}
	}
}

